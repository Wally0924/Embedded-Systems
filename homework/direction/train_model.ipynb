{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szF6m2w9y1HN"
      },
      "source": [
        "# train with pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAPisy3PQMN0"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import math\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU-YjkTCQRDO"
      },
      "outputs": [],
      "source": [
        "myseed = 1234\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rn_5irqxr97"
      },
      "source": [
        "## direction classify model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6J8EFnzIXX4"
      },
      "outputs": [],
      "source": [
        "class direction_classify_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(direction_classify_model, self).__init__()\n",
        "\n",
        "    # TODO change your model & output shape is the total number of your class\n",
        "    # torchvision models\n",
        "    # https://pytorch.org/vision/stable/models.html\n",
        "    self.cnn = torchvision.models.efficientnet_v2_l(weights = \"IMAGENET1K_V1\")\n",
        "    # 四個方向\n",
        "    self.fc = nn.Linear(1000 , 4)\n",
        "    ##########################################\n",
        "  def forward(self, img):\n",
        "\n",
        "    out = self.cnn(img)\n",
        "    out = self.fc(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am7e02WPxy8B"
      },
      "source": [
        "## direction dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30mAHtK7P3ax"
      },
      "outputs": [],
      "source": [
        "# TODO Complete the function\n",
        "class direction_dataset(Dataset):\n",
        "    # 初始化函數，接收文件列表和轉換器\n",
        "    def __init__(self, files, tfm):\n",
        "        super(direction_dataset, self).__init__()\n",
        "        self.files = files  # 存儲文件列表\n",
        "        self.transform = tfm  # 存儲圖像轉換器\n",
        "\n",
        "    # 返回資料集的大小\n",
        "    def __len__(self):\n",
        "        \"\"\"返回資料集的大小\"\"\"\n",
        "        return len(self.files)\n",
        "\n",
        "    # 根據索引 idx 返回圖像張量和類別標籤\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"返回圖像張量和類別標籤\"\"\"\n",
        "        # 打開圖像文件\n",
        "        im = Image.open(self.files[idx][0])\n",
        "        # 使用設定的圖像轉換器進行圖像轉換\n",
        "        im = self.transform(im)\n",
        "        # 返回圖像和對應的類別標籤\n",
        "        return im, self.files[idx][1]\n",
        "##########################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpKfb-vmx4FP"
      },
      "source": [
        "## data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2w8swGXRo8V"
      },
      "outputs": [],
      "source": [
        "# TODO set your img size\n",
        "# torchvision transforming and augmenting images\n",
        "# https://pytorch.org/vision/0.15/transforms.html\n",
        "WIDTH = 0\n",
        "HEIGHT = 0\n",
        "image_size = (HEIGHT , WIDTH)\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "##########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70gnonrHyMJG"
      },
      "source": [
        "## hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJCYZTdkNxmi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# TODO you can adjust hyperparameter\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = direction_classify_model().to(device)\n",
        "batch_size = 32\n",
        "n_epochs = 5\n",
        "patience = 10\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing = 0.05)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
        "_exp_name = \"dircetion_classfication_model\"\n",
        "##########################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szKM5WjkO_5r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO build your dataset\n",
        "train_set = direction_dataset()\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_set = direction_dataset()\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "##########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvMidThbyOEN"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZT974Q-PZ2w"
      },
      "outputs": [],
      "source": [
        "stale = 0\n",
        "best_acc = 0\n",
        "train_acc_record = []\n",
        "train_loss_record = []\n",
        "valid_acc_record = []\n",
        "valid_loss_record = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    # Make sure the model is in train mode before training.\n",
        "    model.train()\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "    for batch in tqdm(train_loader):\n",
        "\n",
        "        imgs , labels = batch\n",
        "\n",
        "        logits = model(imgs.to(device))\n",
        "\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "\n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    train_acc_record.append(train_acc.to('cpu'))\n",
        "    train_loss_record.append(train_loss)\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
        "    model.eval()\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(valid_loader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "\n",
        "        # We don't need gradient in validation.\n",
        "        # Using torch.no_grad() accelerates the forward process.\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        # We can still compute the loss (but not the gradient).\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    valid_acc_record.append(valid_acc.to('cpu'))\n",
        "    valid_loss_record.append(valid_loss)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.pt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe8oJn2YySCI"
      },
      "source": [
        "## train log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BbvYSkZQh17"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([*range(1,len(train_acc_record)+1)] , train_acc_record , label = \"training\")\n",
        "plt.plot([*range(1,len(train_acc_record)+1)] , valid_acc_record , label = \"validation\")\n",
        "\n",
        "plt.xticks(np.arange(0, n_epochs+1, 5))\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.savefig('acc.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot([*range(1,len(train_acc_record)+1)] , train_loss_record , label = \"training\")\n",
        "plt.plot([*range(1,len(train_acc_record)+1)] , valid_loss_record , label = \"valiidation\")\n",
        "\n",
        "plt.xticks(np.arange(0, n_epochs+1, 5))\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.savefig('loss.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JT0cdz9yWGH"
      },
      "source": [
        "# .pt model to .onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYM2CUYs-af4"
      },
      "outputs": [],
      "source": [
        "!pip install onnx\n",
        "# !wget https://npr.brightspotcdn.com/7c/15/1d76bc934e8cb103a56d43eedc7b/sunflower-wide.jpg -O sunflower.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-VZbAaK3rM"
      },
      "source": [
        "## check model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiwbJ770HdDU"
      },
      "outputs": [],
      "source": [
        "# TODO you can check your model or skip this block\n",
        "model = direction_classify_model()\n",
        "model.load_state_dict(torch.load(f\"{_exp_name}_best.pt\"))\n",
        "model.eval()\n",
        "\n",
        "im = Image.open(\"\")\n",
        "im = test_tfm(im)\n",
        "pred = model(im.unsqueeze(0))\n",
        "label = np.argmax(pred.data.numpy(), axis=1)\n",
        "softmax = torch.nn.Softmax(dim = 1)\n",
        "conf = softmax(pred)\n",
        "class_dic = {\n",
        "  0 : \"\",\n",
        "  1 : \"\",\n",
        "  2 : \"\",\n",
        "  3 : \"\",\n",
        "  4 : \"\"\n",
        "}\n",
        "\n",
        "print(class_dic[label[0]] , conf[0][label].item())\n",
        "##########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bDyLKf8K-4_"
      },
      "source": [
        "## export to onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsYWIZQX8AVy"
      },
      "outputs": [],
      "source": [
        "# Input to the model\n",
        "batch_size = 1\n",
        "x = torch.randn(batch_size, 3, HEIGHT , WIDTH , requires_grad=True)\n",
        "torch_out = model(x)\n",
        "# Export the model\n",
        "torch.onnx.export(model,# model being run\n",
        "        x,   # model input (or a tuple for multiple inputs)\n",
        "        f\"{_exp_name}_best.onnx\",  # where to save the model (can be a file or file-like object)\n",
        "        export_params=True, # store the trained parameter weights inside the model file\n",
        "        opset_version=10,   # the ONNX version to export the model to\n",
        "        do_constant_folding=True, # whether to execute constant folding for optimization\n",
        "        input_names = ['input'],  # the model's input names\n",
        "        output_names = ['output'], # the model's output names\n",
        "        dynamic_axes={'input' : {0 : 'batch_size'}, # variable length axes\n",
        "            'output' : {0 : 'batch_size'}})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}